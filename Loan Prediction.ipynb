{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Business problem statement:\n",
    "\n",
    "XYZ Finance company deals in all home loans.They have presence across all urban,semi urban and rural areas.Customer first apply\n",
    "for home loan, after that comapany validates the customer eligibility for loan.Company wants to automate the loan eligibility\n",
    "process(real time) based on customer details provided while filing online application form.These details are Gender,Marital \n",
    "Status,Education,Number of Dependents,Income,Loan Amount,credit history and others.To automate this process, they have given a \n",
    "problem to identify the customer segments, those are eligible for loan amount so that they can specifically target these customers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Machine Learning problem formulation:\n",
    "\n",
    "Loan Business is the main earning busienss of every bank.Loan prediction is the real-life problem faced by every banks.If done\n",
    "correctly,first it can mitigate risk to a particular extent and also save a lot of human hours for a particular bank.\n",
    "\n",
    "It is a classification problem where we have to predit whether a loan would be approved of not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hypothesis Generation:\n",
    "    \n",
    "Below are some of the factors which most probably affect the Loan Approval(Target varaible for this loan prediction problem)\n",
    "\n",
    "* Salary:Applicant with high income should have more chances of the loan approval\n",
    "* Previos History: Applicants who have have repayed their previos loans regulary have higher chances of the loan approval.\n",
    "* Loan amount: Loan approval also depends on the loan amount requested.If the loan amount is less,probability of the approval is high. \n",
    "* Loan Term:If the loan amount and tenure is less, then there is hign chances of approval\n",
    "* EMI: Generally EMI depends on the loan amount and tenure of the loan.If loan amount and tenure(months) is less,ultimately\n",
    "EMI will be less and higher chances of loan approval.\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data:\n",
    "Threre are two files(CSV files):\n",
    "\n",
    "Train file:Train file will be used for training the model. It contains all the features and the target variable\n",
    "    \n",
    "Test File: It contains all the features, but target varaible is not there.This data is used for predicting the class with\n",
    "the help of trained model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading Packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from scipy.stats import zscore\n",
    "from sklearn.preprocessing import LabelEncoder,OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import StratifiedKFold as sfkcv\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import metrics\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading Data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loan_train = pd.read_csv(\"loan_train.csv\")\n",
    "loan_test = pd.read_csv(\"loan_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making a copy of the original data set\n",
    "\n",
    "loan_train_orig = loan_train.copy()\n",
    "loan_test_orig = loan_test.copy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exploratory Data Analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loan_train.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 12 features(independent varaible) and 1 target variable,i.e Loan_Status in the train dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loan_test.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are only 12 features(independent varaibles) in the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data types for each features\n",
    "\n",
    "loan_train.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shape of the data\n",
    "\n",
    "loan_train.shape, loan_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 614 rows and 13 columns in the train dataset and 367 rows and 12 columns in the test dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Univariate Analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Target Varaible:\n",
    "    \n",
    "loan_train.Loan_Status.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# With normalize view(to get the propotions)\n",
    "loan_train.Loan_Status.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loan_train[\"Loan_Status\"].value_counts().plot.bar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loan request of the 422(69%) of people out of 614 was approved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loan_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotting independent varaibles(categorical):\n",
    "\n",
    "plt.figure(1) ;plt.subplot(221); loan_train[\"Gender\"].value_counts(normalize=True).plot.bar(figsize=(20,10),title=\"Gender\")\n",
    "plt.subplot(222);loan_train[\"Married\"].value_counts(normalize=True).plot.bar(title=\"Married\")\n",
    "plt.subplot(223);loan_train[\"Self_Employed\"].value_counts(normalize=True).plot.bar(title=\"self_Employed\")\n",
    "plt.subplot(224);loan_train[\"Credit_History\"].value_counts(normalize=True).plot.bar(title=\"credit_History\")\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observations:\n",
    "    \n",
    "* Out of total number of applicants 80%  are male.\n",
    "* There are around 65% of applicants are married.\n",
    "* There are around 15% of applicants are self employed.\n",
    "* 85% of the applicant have good credit history(They have paid previos loans regulary)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting ordinal independent variables:\n",
    "\n",
    "plt.figure(1)\n",
    "plt.subplot(141);loan_train.Dependents.value_counts(normalize=True).plot.bar(figsize=(24,6),title=\"Dependent\")\n",
    "plt.subplot(142);loan_train.Education.value_counts(normalize=True).plot.bar(title=\"Education\")\n",
    "plt.subplot(143);loan_train.Property_Area.value_counts(normalize=True).plot.bar(title=\"Property_Area\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observations:\n",
    "\n",
    "* Maximum of the applicants do not have any dependent.\n",
    "* Out of total applicants, around 80% are graduates\n",
    "* Most of the applicants belongs to semiurban area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting Numerical features\n",
    "\n",
    "plt.figure(1);plt.subplot(121); sns.distplot(loan_train.ApplicantIncome)\n",
    "plt.subplot(122);sns.boxplot(loan_train.ApplicantIncome,orient='v')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observation:\n",
    "\n",
    "The distribution of the applicant income is not normall distribution and is postive skewed.We will perform column\n",
    "standardization in order to make it a normal distribution.\n",
    "\n",
    "Box plot is indication the presence of a lot of outliers/extreme values.It may be because of the different education level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Coapplicant income distribution\n",
    "\n",
    "plt.figure(1);plt.subplot(121);sns.distplot(loan_train.CoapplicantIncome)\n",
    "plt.subplot(122);sns.boxplot(loan_train.CoapplicantIncome,orient='v')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observation:\n",
    "\n",
    "The distribution of the coapplicant income is also not normal and positive skewed.There are outliers in the incomes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualizing the loan amount distribution\n",
    "\n",
    "plt.figure(1);plt.subplot(121);sns.distplot(loan_train.LoanAmount)\n",
    "plt.subplot(122);sns.boxplot(loan_train.LoanAmount,orient='v')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bivariate Analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen = pd.crosstab(loan_train.Gender,loan_train.Loan_Status).apply(lambda x:x/x.sum() *100,axis=1 )\n",
    "gen.plot.bar(stacked=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "marr = pd.crosstab(loan_train.Married,loan_train.Loan_Status).apply(lambda x:x/x.sum() *100,axis=1 )\n",
    "marr.plot.bar(stacked=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dep = pd.crosstab(loan_train.Dependents,loan_train.Loan_Status).apply(lambda x:x/x.sum() *100,axis=1 )\n",
    "dep.plot.bar(stacked=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edu = pd.crosstab(loan_train.Education,loan_train.Loan_Status).apply(lambda x:x/x.sum() *100,axis=1 )\n",
    "edu.plot.bar(stacked=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "se = pd.crosstab(loan_train.Self_Employed,loan_train.Loan_Status).apply(lambda x:x/x.sum() *100,axis=1 )\n",
    "se.plot.bar(stacked=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observations:\n",
    "    \n",
    "* Loan was approved for the majority of applicant who were married.\n",
    "* Applicants having dependent 1 and 3+ falls in the same categories with respect to Loan status."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For credit History and Property areas \n",
    "\n",
    "credit_his = pd.crosstab(loan_train.Credit_History,loan_train.Loan_Status).apply(lambda x:x/x.sum() *100,axis=1 )\n",
    "credit_his.plot.bar(stacked=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prop_area = pd.crosstab(loan_train.Property_Area,loan_train.Loan_Status).apply(lambda x:x/x.sum() *100,axis=1 )\n",
    "prop_area.plot.bar(stacked=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observations:\n",
    "    \n",
    "* People with good credit history(1) are having more changes of loan approval #  Hypothesis holds true\n",
    "* Loans associated with semiurban areas having major proportion in the approved category."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Numerical Feature and target variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loan_train.groupby(\"Loan_Status\").agg({\"ApplicantIncome\":'mean'}).plot.bar()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above we say that mean income is coming same for the both categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorising Applicants income into different labels based on the values.\n",
    "\n",
    "bins = [0,2500,4000,6000,np.inf]\n",
    "labels = [\"Low\",\"Average\",\"High\",\"Vey high\"]\n",
    "loan_train[\"Income_category\"]= pd.cut(loan_train.ApplicantIncome,bins=bins,labels=labels)\n",
    "income_category = pd.crosstab(loan_train.Income_category,loan_train.Loan_Status).apply(lambda x:x/x.sum() *100,axis=1 )\n",
    "income_category.plot.bar(stacked=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above, it can be inferred that applicant income does not affect the possibilty of the loan approval, which is \n",
    "contradicting our hypothesis that high income results in high chances of loan approval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analysis of the coapplicant income.\n",
    "\n",
    "bins = [0,2500,4000,6000,np.inf]\n",
    "labels = [\"Low\",\"Average\",\"High\",\"Vey high\"]\n",
    "loan_train[\"Coapplicant_Income_category\"]= pd.cut(loan_train.CoapplicantIncome,bins=bins,labels=labels)\n",
    "coapplicant_income_category = pd.crosstab(loan_train.Coapplicant_Income_category,loan_train.Loan_Status).apply(lambda x:x/x.sum() *100,axis=1 )\n",
    "coapplicant_income_category.plot.bar(stacked=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above plot, it reflects that if the coapplicant income is less the chances of the laon approval is more.But in reality\n",
    "it does not look good.Here it may be possible because majority of the applicant does not have coapplicat and thus income is taken \n",
    "0 and so the loan approval is not dependent on it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So in order to get the clear picture combine applicant income and coapplicant income and interpret the combine effect on \n",
    "loan approval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loan_train[\"total_income\"]= loan_train.ApplicantIncome + loan_train.CoapplicantIncome\n",
    "\n",
    "bins = [0,2500,4000,6000,np.inf]\n",
    "labels = [\"Low\",\"Average\",\"High\",\"Vey high\"]\n",
    "loan_train[\"Total_Income_category\"]= pd.cut(loan_train.total_income,bins=bins,labels=labels)\n",
    "total_income_category = pd.crosstab(loan_train.Total_Income_category,loan_train.Loan_Status).apply(lambda x:x/x.sum() *100,axis=1 )\n",
    "total_income_category.plot.bar(stacked=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is observed that in the low(Total income) category, proportion of the loan approval is less as compared to other three categories."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analysis of the Loan amount feature:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = [0,150,250,650]\n",
    "labels = [\"Low\",\"Average\",\"High\"]\n",
    "loan_train[\"loan_amount_category\"]= pd.cut(loan_train.LoanAmount,bins=bins,labels=labels)\n",
    "loan_amount_category = pd.crosstab(loan_train.loan_amount_category,loan_train.Loan_Status).apply(lambda x:x/x.sum() *100,axis=1 )\n",
    "loan_amount_category.plot.bar(stacked=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is observed that for the low and average category with resepect the loan amount,proportion of the approved loans is higher\n",
    "as compare to the high category.It is favouring our hypothesis that loan approval changes are high for the less loan amount."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Droping the extra added columns:\n",
    "loan_train = loan_train.drop([\"Income_category\",\"Coapplicant_Income_category\",\"total_income\",\"Total_Income_category\",\n",
    "                               \"loan_amount_category\"],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loan_train.Dependents.replace('3+',3,inplace=True)# replacing \"3+\" with 3\n",
    "loan_test.Dependents.replace('3+',3,inplace=True)\n",
    "loan_train.Loan_Status.replace('N',0,inplace=True)\n",
    "loan_train.Loan_Status.replace('Y',1,inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_matrix = loan_train.corr()\n",
    "#sns.heatmap(corr_matrix,vmin=-1.0,vmax=1,cmap=\"BuPu\",square=True)\n",
    "sns.heatmap(corr_matrix,annot =True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Observation:\n",
    "* Most correlated features are (Applicantincome-LoanAmount) and (Credit_History-Loan_Status)\n",
    "* Loan_Amount is also correlated to the Coapplicant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Preprocessing\n",
    "\n",
    "loan_train.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Features like Gender,Married,Dependent,Self_Employed,Loan_Amount,Loan_Amount_Term and Credit_History are having missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Missing value imputation for categorical features\n",
    "loan_train.Gender.fillna(loan_train.Gender.mode()[0],inplace=True)\n",
    "loan_train.Married.fillna(loan_train.Married.mode()[0],inplace=True)\n",
    "loan_train.Dependents.fillna(loan_train.Dependents.mode()[0],inplace=True)\n",
    "loan_train.Self_Employed.fillna(loan_train.Self_Employed.mode()[0],inplace=True)\n",
    "loan_train.Credit_History.fillna(loan_train.Credit_History.mode()[0],inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since feature LoanAmount have most number of outliers,so mean will not be robust in this case.Therefore we will use median\n",
    "#for the missing value imputation\n",
    "\n",
    "loan_train.LoanAmount.fillna(loan_train.LoanAmount.median(),inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loan_train.Loan_Amount_Term.fillna(loan_train.Loan_Amount_Term.mode()[0],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "loan_test.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Missing value imputation for test dataset.\n",
    "\n",
    "loan_test.Gender.fillna(loan_test.Gender.mode()[0],inplace=True)\n",
    "loan_test.Dependents.fillna(loan_test.Dependents.mode()[0],inplace=True)\n",
    "loan_test.Self_Employed.fillna(loan_test.Self_Employed.mode()[0],inplace=True)\n",
    "loan_test.Credit_History.fillna(loan_test.Credit_History.mode()[0],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for numerical features\n",
    "loan_test.LoanAmount.fillna(loan_test.LoanAmount.median(),inplace = True)\n",
    "loan_test.Loan_Amount_Term.fillna(loan_test.Loan_Amount_Term.mode()[0],inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# making dummmy variables for the categorical features\n",
    "loan_train=loan_train.drop(\"Loan_ID\",axis=1)\n",
    "loan_test = loan_test.drop(\"Loan_ID\",axis=1)\n",
    "\n",
    "#loan_train1= pd.get_dummies(loan_train)\n",
    "#loan_test1=pd.get_dummies(loan_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# label encoding train data\n",
    "le = LabelEncoder()\n",
    "loan_train.Gender = le.fit_transform(loan_train.Gender)\n",
    "loan_train.Married = le.fit_transform(loan_train.Married)\n",
    "loan_train.Education = le.fit_transform(loan_train.Education)\n",
    "loan_train.Self_Employed = le.fit_transform(loan_train.Self_Employed)\n",
    "loan_train.Property_Area = le.fit_transform(loan_train.Property_Area)\n",
    "loan_train.Dependents = loan_train.Dependents.astype(int) # changing datatype of the Dependent feature.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label Encoding test data\n",
    "le = LabelEncoder()\n",
    "loan_test.Gender = le.fit_transform(loan_test.Gender)\n",
    "loan_test.Married = le.fit_transform(loan_test.Married)\n",
    "loan_test.Education = le.fit_transform(loan_test.Education)\n",
    "loan_test.Self_Employed = le.fit_transform(loan_test.Self_Employed)\n",
    "loan_test.Property_Area = le.fit_transform(loan_test.Property_Area)\n",
    "loan_test.Dependents = loan_test.Dependents.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(614, 12)\n",
      "(577, 12)\n"
     ]
    }
   ],
   "source": [
    "# removing the outliers for train dataset\n",
    "\n",
    "z_score = abs(zscore(loan_train))\n",
    "print(loan_train.shape)\n",
    "loan_train_final = loan_train.loc[(z_score<3).all(axis=1)]\n",
    "print(loan_train_final.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q1 = loan_train1.quantile(0.25)\n",
    "#Q3 = loan_train1.quantile(0.75)\n",
    "#IQR = Q3 - Q1\n",
    "#print(IQR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loan1 = loan_train1[~((loan_train1 < (Q1 - 1.5 * IQR)) |(loan_train1 > (Q3 + 1.5 * IQR))).any(axis=1)]\n",
    "#loan1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing the outliers for testdataset\n",
    "\n",
    "#z_score1 = abs(zscore(loan_test))\n",
    "#print(loan_test.shape)\n",
    "#loan_test_final = loan_test.loc[(z_score1<3).all(axis=1)]\n",
    "#print(loan_test_final.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seperating features and target varaible\n",
    "\n",
    "x = loan_train_final.iloc[:,0:-1]\n",
    "y = loan_train_final.iloc[:,-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scaling down using standardscaler\n",
    "sc = StandardScaler()\n",
    "x = sc.fit_transform(x) # train data \n",
    "loan_test_scaled = sc.fit_transform(loan_test)\n",
    "x = pd.DataFrame(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train,x_val,y_train,y_val = train_test_split(x,y,test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression()\n",
    "model.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model.predict(x_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8333333333333334"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_val,pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here accuracy is coming around 83%.It means 83% of the loan status is correctly identified."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now doing the pediction for the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_final = model.predict(loan_test_scaled)\n",
    "test_loan_status = pd.DataFrame(pred_final)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing the submission file\n",
    "\n",
    "submission1 = pd.read_csv(\"sample_submission_49d68Cx.csv\")\n",
    "submission1[\"Loan_Status\"] = test_loan_status\n",
    "submission1[\"Loan_ID\"] = loan_test_orig[\"Loan_ID\"]\n",
    "submission1[\"Loan_Status\"].replace(0,'N',inplace= True)\n",
    "submission1[\"Loan_Status\"].replace(1,'Y',inplace= True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Converting submission1 to .csv file\n",
    "\n",
    "pd.DataFrame(submission1,columns=['Loan_ID','Loan_Status']).to_csv('LR1.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taking varios models\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "gnb = GaussianNB()\n",
    "lg = LogisticRegression()\n",
    "dt = DecisionTreeClassifier()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "for: KNeighborsClassifier\n",
      "for k =1,accuracy is 0.7758620689655172\n",
      "for k =2,accuracy is 0.7758620689655172\n",
      "for k =3,accuracy is 0.782608695652174\n",
      "for k =4,accuracy is 0.8434782608695652\n",
      "for k =5,accuracy is 0.782608695652174\n",
      "Mean accuarcy score = 0.7920839580209895\n",
      "\n",
      "\n",
      "for: GaussianNB\n",
      "for k =1,accuracy is 0.8103448275862069\n",
      "for k =2,accuracy is 0.7931034482758621\n",
      "for k =3,accuracy is 0.7913043478260869\n",
      "for k =4,accuracy is 0.8260869565217391\n",
      "for k =5,accuracy is 0.808695652173913\n",
      "Mean accuarcy score = 0.8059070464767617\n",
      "\n",
      "\n",
      "for: LogisticRegression\n",
      "for k =1,accuracy is 0.8103448275862069\n",
      "for k =2,accuracy is 0.7931034482758621\n",
      "for k =3,accuracy is 0.7913043478260869\n",
      "for k =4,accuracy is 0.8608695652173913\n",
      "for k =5,accuracy is 0.8260869565217391\n",
      "Mean accuarcy score = 0.8163418290854573\n",
      "\n",
      "\n",
      "for: DecisionTreeClassifier\n",
      "for k =1,accuracy is 0.7413793103448276\n",
      "for k =2,accuracy is 0.7068965517241379\n",
      "for k =3,accuracy is 0.7043478260869566\n",
      "for k =4,accuracy is 0.7478260869565218\n",
      "for k =5,accuracy is 0.7043478260869566\n",
      "Mean accuarcy score = 0.7209595202398801\n",
      "\n",
      "\n",
      "[0.7920839580209895, 0.8059070464767617, 0.8163418290854573, 0.7209595202398801]\n",
      "Model with highest accuracy:  LogisticRegression\n"
     ]
    }
   ],
   "source": [
    "# Stratified k-fold cross validation\n",
    "best_model=[]\n",
    "best_model_name=[\"KNeighborsClassifier\",\"GaussianNB\",\"LogisticRegression\",\"DecisionTreeClassifier\"]\n",
    "count =0\n",
    "for model in [knn,gnb,lg,dt]:\n",
    "   \n",
    "    accuracy = []\n",
    "    exp=0\n",
    "    skf = sfkcv(n_splits=5,random_state=None)\n",
    "    print(\"\\n\")\n",
    "    print(\"for:\",best_model_name[count])\n",
    "    for train_index,test_index in skf.split(x,y):\n",
    "        exp+=1\n",
    "         #print(\"Train Indexes\",train_index,\"Validation indexes\",test_index)\n",
    "        #print(\"Experiment no:\",exp)\n",
    "        #print(\"Lenght of Train Set:\",len(train_index))\n",
    "        #print(\"Lenth of Test set: \", len(test_index))\n",
    "        X_train,X_test=x.iloc[train_index],x.iloc[test_index]\n",
    "        Y_train,Y_test=y.iloc[train_index],y.iloc[test_index]\n",
    "        #print(Y_train.value_counts())\n",
    "        #print(Y_test.value_counts())\n",
    "        model.fit(X_train,Y_train)\n",
    "        Y_predict=model.predict(X_test)\n",
    "        score = metrics.accuracy_score(Y_test,Y_predict)\n",
    "        print(\"for k ={},accuracy is {}\".format(exp,score))\n",
    "        accuracy.append(score)\n",
    "    #print(accuracy)\n",
    "    print(\"Mean accuarcy score =\",np.array(accuracy).mean())\n",
    "    best_model.append(np.array(accuracy).mean())\n",
    "    count=count+1\n",
    "#print(best_model)\n",
    "best_index = best_model.index(max(best_model))\n",
    "print(\"\\n\")\n",
    "print(best_model)\n",
    "print(\"Model with highest accuracy: \",best_model_name[best_index])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now doing the pediction for the test data with LogisticRegression using stratified k-folds cross validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean validation accuracy for the model: 0.8163418290854573\n"
     ]
    }
   ],
   "source": [
    "accuracy = []\n",
    "exp=0\n",
    "skf = sfkcv(n_splits=5,random_state=None)\n",
    "for train_index,test_index in skf.split(x,y):\n",
    "    exp+=1\n",
    "         \n",
    "    X_train,X_test=x.iloc[train_index],x.iloc[test_index]\n",
    "    Y_train,Y_test=y.iloc[train_index],y.iloc[test_index]\n",
    "    #print(Y_train.value_counts())\n",
    "    #print(Y_test.value_counts())\n",
    "    lr = LogisticRegression()\n",
    "    lr.fit(X_train,Y_train)\n",
    "    Y_predict=lg.predict(X_test)\n",
    "    score = metrics.accuracy_score(Y_test,Y_predict)\n",
    "    accuracy.append(score)\n",
    "    pred_test = lr.predict(loan_test_scaled)\n",
    "    #pred = lr.predict_proba(X_test)[:,1]\n",
    "    test_loan_status1 = pd.DataFrame(pred_final)\n",
    "print(\"Mean validation accuracy for the model:\",np.mean(accuracy))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing submission file \n",
    "submission2 = pd.read_csv(\"sample_submission_49d68Cx.csv\")\n",
    "submission2[\"Loan_Status\"] = test_loan_status1\n",
    "submission2[\"Loan_ID\"] = loan_test_orig[\"Loan_ID\"]\n",
    "submission2[\"Loan_Status\"].replace(0,'N',inplace= True)\n",
    "submission2[\"Loan_Status\"].replace(1,'Y',inplace= True)\n",
    "\n",
    "#Convrting submission1 to .csv file\n",
    "\n",
    "pd.DataFrame(submission2,columns=['Loan_ID','Loan_Status']).to_csv('LR(with stratified k fold).csv',index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 40 candidates, totalling 120 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  56 tasks      | elapsed:    0.4s\n",
      "[Parallel(n_jobs=-1)]: Done 120 out of 120 | elapsed:    0.5s finished\n"
     ]
    }
   ],
   "source": [
    "# Now with the help of grid search we will do hyperparameter tunning for improving the accuracy.\n",
    "# KNN Classifier Hyperparametr\n",
    "weight_options = ['uniform', 'distance']\n",
    "k_range = list(range(1,20,2))\n",
    "grid_params = {'n_neighbors':k_range,\n",
    "               'weights':weight_options,\n",
    "                'metric':['euclidean','manhattan']\n",
    "              }\n",
    "\n",
    "GS =GridSearchCV(\n",
    "     KNeighborsClassifier(),\n",
    "     grid_params,\n",
    "     verbose=1,\n",
    "     cv =3,\n",
    "     n_jobs =-1\n",
    "     )\n",
    "GS_results = GS.fit(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal accuracy: 0.811078727691422\n"
     ]
    }
   ],
   "source": [
    "print(\"Optimal accuracy:\",GS_results.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='euclidean',\n",
       "                     metric_params=None, n_jobs=None, n_neighbors=9, p=2,\n",
       "                     weights='uniform')"
      ]
     },
     "execution_count": 348,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GS_results.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal Parameter: {'metric': 'euclidean', 'n_neighbors': 9, 'weights': 'uniform'}\n"
     ]
    }
   ],
   "source": [
    "print(\"Optimal Parameter:\",GS_results.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 20 candidates, totalling 60 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    3.0s\n",
      "[Parallel(n_jobs=-1)]: Done  60 out of  60 | elapsed:    3.1s finished\n"
     ]
    }
   ],
   "source": [
    "# Decision Tree Hyperparatmeter tunning\n",
    "grid_params = {'max_depth':list(range(1,20,2)),\n",
    "                'criterion':['gini','entropy']\n",
    "              }\n",
    "\n",
    "GS =GridSearchCV(\n",
    "     DecisionTreeClassifier(),\n",
    "     grid_params,\n",
    "     verbose=1,\n",
    "     cv =3,\n",
    "     n_jobs =-1\n",
    "     )\n",
    "GS_results = GS.fit(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal accuracy: 0.811078727691422\n"
     ]
    }
   ],
   "source": [
    "print(\"Optimal accuracy:\",GS_results.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',\n",
       "                       max_depth=1, max_features=None, max_leaf_nodes=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=1, min_samples_split=2,\n",
       "                       min_weight_fraction_leaf=0.0, presort='deprecated',\n",
       "                       random_state=None, splitter='best')"
      ]
     },
     "execution_count": 330,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GS_results.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal Parameter: {'metric': 'euclidean', 'n_neighbors': 9, 'weights': 'uniform'}\n"
     ]
    }
   ],
   "source": [
    "print(\"Optimal Parameter:\",GS_results.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 100 candidates, totalling 300 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    5.0s\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:   13.9s\n",
      "[Parallel(n_jobs=-1)]: Done 300 out of 300 | elapsed:   21.1s finished\n"
     ]
    }
   ],
   "source": [
    "# Random Forest Classifier Hyperparatmeter tunning\n",
    "max_depth =list(range(1,20,2))\n",
    "n_estimators = [1,21,41,61,81,101,121,141,161,181]\n",
    "grid_params = {'max_depth':max_depth,\n",
    "               'n_estimators':n_estimators\n",
    "              }\n",
    "\n",
    "GS =GridSearchCV(\n",
    "    RandomForestClassifier(),\n",
    "     grid_params,\n",
    "     verbose=1,\n",
    "     cv =3,\n",
    "     n_jobs =-1\n",
    "     )\n",
    "GS_results = GS.fit(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal accuracy: 0.8180411629245826\n",
      "Optimal Parameter: {'max_depth': 3, 'n_estimators': 61}\n"
     ]
    }
   ],
   "source": [
    "print(\"Optimal accuracy:\",GS_results.best_score_)\n",
    "print(\"Optimal Parameter:\",GS_results.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy= 0.8793103448275862\n"
     ]
    }
   ],
   "source": [
    "# making the prediction with decision tree('criterion': 'gini', 'max_depth': 1)\n",
    "\n",
    "x_train,x_val,y_train,y_val = train_test_split(x,y,test_size=0.3)\n",
    "model = DecisionTreeClassifier(max_depth=1)\n",
    "model.fit(x_train,y_train)\n",
    "pred = model.predict(x_val)\n",
    "print(\"Accuracy=\",accuracy_score(y_val,pred))\n",
    "\n",
    "#Now doing the pediction for the test data.\n",
    "\n",
    "pred_final1 = model.predict(loan_test_scaled)\n",
    "test_loan_status1 = pd.DataFrame(pred_final1)\n",
    "\n",
    "#Importing the submission file\n",
    "\n",
    "submission2 = pd.read_csv(\"sample_submission_49d68Cx.csv\")\n",
    "submission2[\"Loan_Status\"] = test_loan_status1\n",
    "submission2[\"Loan_ID\"] = loan_test_orig[\"Loan_ID\"]\n",
    "submission2[\"Loan_Status\"].replace(0,'N',inplace= True)\n",
    "submission2[\"Loan_Status\"].replace(1,'Y',inplace= True)\n",
    "\n",
    "#Converting submission1 to .csv file\n",
    "\n",
    "pd.DataFrame(submission2,columns=['Loan_ID','Loan_Status']).to_csv('Decision Tree1.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy= 0.7988505747126436\n"
     ]
    }
   ],
   "source": [
    "# making the prediction with KNeighborsClassifier('metric': 'euclidean', 'n_neighbors': 9, 'weights': 'uniform')\n",
    "\n",
    "x_train,x_val,y_train,y_val = train_test_split(x,y,test_size=0.3)\n",
    "model = knn = KNeighborsClassifier(n_neighbors=9,metric='euclidean')\n",
    "model.fit(x_train,y_train)\n",
    "pred = model.predict(x_val)\n",
    "print(\"Accuracy=\",accuracy_score(y_val,pred))\n",
    "\n",
    "#Now doing the pediction for the test data.\n",
    "\n",
    "pred_final2 = model.predict(loan_test_scaled)\n",
    "test_loan_status2 = pd.DataFrame(pred_final2)\n",
    "\n",
    "#Importing the submission file\n",
    "\n",
    "submission3 = pd.read_csv(\"sample_submission_49d68Cx.csv\")\n",
    "submission3[\"Loan_Status\"] = test_loan_status2\n",
    "submission3[\"Loan_ID\"] = loan_test_orig[\"Loan_ID\"]\n",
    "submission3[\"Loan_Status\"].replace(0,'N',inplace= True)\n",
    "submission3[\"Loan_Status\"].replace(1,'Y',inplace= True)\n",
    "\n",
    "#Converting submission1 to .csv file\n",
    "\n",
    "pd.DataFrame(submission3,columns=['Loan_ID','Loan_Status']).to_csv('KNN Classifier.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy= 0.8390804597701149\n"
     ]
    }
   ],
   "source": [
    "# making the prediction with RandomForestClassifier('max_depth': 3, 'n_estimators': 61)\n",
    "\n",
    "x_train,x_val,y_train,y_val = train_test_split(x,y,test_size=0.3)\n",
    "model = knn = RandomForestClassifier(max_depth=3,n_estimators=63)\n",
    "model.fit(x_train,y_train)\n",
    "pred = model.predict(x_val)\n",
    "print(\"Accuracy=\",accuracy_score(y_val,pred))\n",
    "\n",
    "#Now doing the pediction for the test data.\n",
    "\n",
    "pred_final3 = model.predict(loan_test_scaled)\n",
    "test_loan_status3 = pd.DataFrame(pred_final2)\n",
    "\n",
    "#Importing the submission file\n",
    "\n",
    "submission4 = pd.read_csv(\"sample_submission_49d68Cx.csv\")\n",
    "submission4[\"Loan_Status\"] = test_loan_status3\n",
    "submission4[\"Loan_ID\"] = loan_test_orig[\"Loan_ID\"]\n",
    "submission4[\"Loan_Status\"].replace(0,'N',inplace= True)\n",
    "submission4[\"Loan_Status\"].replace(1,'Y',inplace= True)\n",
    "\n",
    "#Converting submission1 to .csv file\n",
    "\n",
    "pd.DataFrame(submission4,columns=['Loan_ID','Loan_Status']).to_csv('RandomForest Classifier.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
